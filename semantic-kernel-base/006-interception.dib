#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"}]}}

#!csharp

#r "nuget: Microsoft.SemanticKernel"
#r "nuget: Microsoft.Extensions.Logging"
#r "nuget: Microsoft.Extensions.Logging.Console"
#r "nuget: Microsoft.Extensions.Logging.Debug"
#r "nuget: Microsoft.Extensions.Http"

#!import ..\dotenv.cs
#!import ..\commonFull.cs

#!csharp

// Now configure kernel memory
using Microsoft.SemanticKernel;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;

private static DumpLoggingProvider _loggingProvider = new DumpLoggingProvider();

var kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.Services.AddLogging(l => l
    .SetMinimumLevel(LogLevel.Trace)
    .AddConsole()
    .AddDebug()
    .AddProvider(_loggingProvider)
);

kernelBuilder.Services.ConfigureHttpClientDefaults(c => c
    .AddLogger(s => _loggingProvider.CreateHttpRequestBodyLogger(s.GetRequiredService<ILogger<DumpLoggingProvider>>())));

kernelBuilder.Services.AddAzureOpenAIChatCompletion(
    "GPT4o", //"GPT35_2",//"GPT42",
    Dotenv.Get("OPENAI_API_BASE"),
    Dotenv.Get("OPENAI_API_KEY"),
    serviceId: "default",
    modelId: "gpt4o");

#!csharp

var kernel = kernelBuilder.Build();
var result = await kernel.InvokePromptAsync("Which is the capital of France. Answer as pirate Barbossa!!!");
Console.WriteLine(result);

#!csharp

var calls = _loggingProvider.GetLLMCalls();

Console.WriteLine("Number of Calls: {0}", calls.Count());
foreach (LLMCall call in calls)
{
    Console.WriteLine("Url: " + call.Url);
    Console.WriteLine("FullPrompt:\n" + call.FullRequest + "\n\n");
    Console.WriteLine("ResponseFunctionCall: " + call.ResponseFunctionCall);
    Console.WriteLine("Response: " + call.Response);
}
