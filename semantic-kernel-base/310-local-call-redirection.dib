#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"name":"csharp","languageName":"C#","aliases":["c#","cs"]},{"name":"fsharp","languageName":"F#","aliases":["f#","fs"]},{"name":"html","languageName":"HTML"},{"name":"http","languageName":"HTTP"},{"name":"javascript","languageName":"JavaScript","aliases":["js"]},{"name":"kql","languageName":"KQL"},{"name":"mermaid","languageName":"Mermaid"},{"name":"pwsh","languageName":"PowerShell","aliases":["powershell"]},{"name":"sql","languageName":"SQL"},{"name":"value"}]}}

#!csharp

#!import ../dotenv.cs
#r "nuget: Microsoft.SemanticKernel"
#r "nuget: Microsoft.Extensions.Logging"
#r "nuget: Microsoft.Extensions.Logging.Console"
#r "nuget: Microsoft.Extensions.Logging.Debug"

#!markdown

Now we declare an HttpClient Handler that is capable of `redirecting the call to api.openai.com to local LM Studio instance`. This technique has the advantage of working without direct support of Semantic Kernel or other libraries.

#!csharp

using System.Net.Http;
using System.Threading;

private class ProxyOpenAIHandler : HttpClientHandler
{
    protected override Task<HttpResponseMessage> SendAsync(
        HttpRequestMessage request, 
        CancellationToken cancellationToken)
    {
        if (request.RequestUri != null 
            && request.RequestUri.Host.Equals("api.openai.com", StringComparison.OrdinalIgnoreCase))
        {
            // Redirect to your local LLM server
            request.RequestUri = new Uri($"http://localhost:1234{request.RequestUri.PathAndQuery}");
        }
        return base.SendAsync(request, cancellationToken);
    }
}

#!markdown

Now you configure `openai api access` but you can avoid specifiyng API key or other paramters except passing the HttpClient with the handler created above

#!csharp

// Now configure kernel memory
using Microsoft.SemanticKernel;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;

// Register the HttpClient in the KernelBuilder's services
var kernelBuilder = Kernel.CreateBuilder();
//kernelBuilder.Services.AddSingleton<HttpClient>(httpClient);

kernelBuilder.Services.AddLogging(l => l
    .SetMinimumLevel(LogLevel.Warning)
    .AddConsole()
    .AddDebug()
);
var kernel = kernelBuilder
    .AddOpenAIChatCompletion(
        "gemma-3-4b-it", 
        apiKey : "xxxx", 
        serviceId: "gemma-3-4b-it",
        httpClient: new HttpClient(new ProxyOpenAIHandler())
    { 
        Timeout = TimeSpan.FromMinutes(10)      
    })
    .Build();

#!markdown

Now interact as usual with Semantic Kernel library.

#!csharp

var kernel = kernelBuilder.Build();
var result = await kernel.InvokePromptAsync("Which is the capital of France. Answer as pirate Barbossa!!!");
Console.WriteLine(result);
