#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"}]}}

#!csharp

#r "nuget: Microsoft.Extensions.Logging"
#r "nuget: Microsoft.SemanticKernel"
#r "nuget: Microsoft.Extensions.Logging.Console"
#r "nuget: Microsoft.Extensions.Logging.Debug"
#r "nuget: Microsoft.Extensions.Http"
#r "nuget: Microsoft.SemanticKernel.PromptTemplates.Handlebars"
#r "nuget: Microsoft.Extensions.DependencyInjection.Abstractions"

#!import ../dotenv.cs
#!import ../pythonWrapper.cs
#!import ../commonFullDi.cs
#!import plugins/AudioVideoPlugin/AudioVideo.cs

 Console.WriteLine($"Framework Description: {System.Runtime.InteropServices.RuntimeInformation.FrameworkDescription}");

#!csharp

var kernelBuilder = Common.ConfigureKernelBuilder(enableLogging: true, enableDumpProvider: true);

var audioVideoPluginConfig = new AudioVideoPluginConfig("setting");
Common.Services.AddSingleton(audioVideoPluginConfig);  

kernelBuilder
    .Plugins
        .AddFromType<AudioVideoPlugin>("AudioVideoPlugin");

#!csharp

var kernel = Common.Resolve<Kernel>();

#!csharp

using Microsoft.SemanticKernel.Connectors.OpenAI;
OpenAIPromptExecutionSettings openAIPromptExecutionSettings = new()
{
    ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions,
    Temperature = 0,
};

#!csharp

using Microsoft.SemanticKernel.ChatCompletion;
var r = Common.Resolve<IChatCompletionService>();

#!csharp

Common.DumpLoggingProvider.ClearLLMCalls();
var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();
var videoFile = "c:\\temp\\ssh.mp4";
ChatHistory chatMessages = new();
chatMessages.AddUserMessage($"I want to extract audio from video file {videoFile}");
var results = await chatCompletionService.GetChatMessageContentsAsync(
        chatMessages,
        executionSettings: openAIPromptExecutionSettings,
        kernel: kernel);   

Console.WriteLine(results);  

#!csharp

using System.Text.Json;

Console.WriteLine("Results count: {0}", results.Count);
//dump all result types
var options = new JsonSerializerOptions
{
    WriteIndented = true
};

foreach (OpenAIChatMessageContent result in results)
{
    Console.WriteLine("{0} = {1}", result.GetType(), result);
    Console.WriteLine(JsonSerializer.Serialize(result, options));
}

#!csharp

var llmCalls = Common.DumpLoggingProvider.GetLLMCalls();
int requestCount = 1;
foreach (var llmCall in llmCalls)
{
    Console.WriteLine("\n----------------- Request {0} -----------------\n", requestCount);
    JsonDocument requestJsonDocument = JsonDocument.Parse(llmCall.FullRequest);        
    string requestJson = JsonSerializer.Serialize(requestJsonDocument, new JsonSerializerOptions { WriteIndented = true });
    Console.WriteLine(requestJson);
    Console.WriteLine("\n----------------- Response {0} -----------------\n", requestCount++);
    
    JsonDocument responseJsonDocument = JsonDocument.Parse(llmCall.Response);
    string responseJson =  JsonSerializer.Serialize(responseJsonDocument, new JsonSerializerOptions { WriteIndented = true });
    if (llmCall.ResponseFunctionCall != null)
    {
        //This is a function call, the LLM answer and tell us to call a plugin, it will invoked automatically
        Console.WriteLine($"Function call: {llmCall.ResponseFunctionCall} with arguments {llmCall.ResponseFunctionCallParameters}");
    }
    else
    {
        //we have a simple call we can dump the response
        //we can deserialize json into an object and get the content
        var response = (JsonElement) JsonSerializer.Deserialize<dynamic>(llmCall.Response);
        var choices = response.GetProperty("choices");
        var firstChoice = choices[0];
        Console.WriteLine("Message Response {0}:", firstChoice.GetProperty("message"));
    }

    Console.WriteLine("Full Response:\n{0}", responseJson);
}
